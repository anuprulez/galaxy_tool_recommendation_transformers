{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ffbe4a-b59d-40e1-a90c-6c67be989a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create transformers\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "fig_size = (15, 15)\n",
    "font = {'family': 'serif', 'size': 8}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "batch_size = 100\n",
    "test_batches = 1\n",
    "n_topk = 1\n",
    "max_seq_len = 25\n",
    "\n",
    "embed_dim = 128 # Embedding size for each token d_model\n",
    "num_heads = 4 # Number of attention heads\n",
    "ff_dim = 128 # Hidden layer size in feed forward network inside transformer # dff\n",
    "dropout = 0.1\n",
    "seq_len = 25\n",
    "\n",
    "predict_rnn = False\n",
    "\n",
    "if predict_rnn is True:\n",
    "    base_path = \"../models/rnn/\"\n",
    "    #\"log_19_09_22_GPU_RNN_full_data/\"\n",
    "    #\"/media/anupkumar/b1ea0d39-97af-4ba5-983f-cd3ff76cf7a6/tool_prediction_datasets/computed_results/aug_22 data/rnn/run2/\" #\"log_19_09_22_GPU_RNN_full_data/\" #\"log_22_08_22_rnn/\" #\"log_08_08_22_rnn/\"\n",
    "else:\n",
    "    base_path = \"../models/transformer/\"\n",
    "    #\"log_19_09_22_GPU_transformer_full_data/\" #\"log_12_09_22_GPU/\" #\"log_19_09_22_GPU_transformer_full_data/\" \n",
    "\n",
    "model_number = 40000\n",
    "model_path = base_path + \"saved_model/\" + str(model_number) + \"/tf_model/\"\n",
    "model_path_h5 = base_path + \"saved_model/\" + str(model_number) + \"/tf_model_h5/\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=rate)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output, attention_scores = self.att(inputs, inputs, inputs, return_attention_scores=True, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output), attention_scores\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        file_content = json.loads(json_file.read())\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def write_file(file_path, content):\n",
    "    \"\"\"\n",
    "    Write a file\n",
    "    \"\"\"\n",
    "    remove_file(file_path)\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json_file.write(json.dumps(content))\n",
    "\n",
    "def create_rnn_model(seq_len, vocab_size):\n",
    "\n",
    "    seq_inputs = tf.keras.Input(batch_shape=(None, seq_len))\n",
    "\n",
    "    gen_embedding = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)\n",
    "    in_gru = tf.keras.layers.GRU(ff_dim, return_sequences=True, return_state=False)\n",
    "    out_gru = tf.keras.layers.GRU(ff_dim, return_sequences=False, return_state=True)\n",
    "    enc_fc = tf.keras.layers.Dense(vocab_size, activation='sigmoid', kernel_regularizer=\"l2\")\n",
    "\n",
    "    embed = gen_embedding(seq_inputs)\n",
    "\n",
    "    embed = tf.keras.layers.Dropout(dropout)(embed)\n",
    "\n",
    "    gru_output = in_gru(embed)\n",
    "\n",
    "    gru_output = tf.keras.layers.Dropout(dropout)(gru_output)\n",
    "\n",
    "    gru_output, hidden_state = out_gru(gru_output)\n",
    "\n",
    "    gru_output = tf.keras.layers.Dropout(dropout)(gru_output)\n",
    "\n",
    "    fc_output = enc_fc(gru_output)\n",
    "\n",
    "    return Model(inputs=[seq_inputs], outputs=[fc_output])\n",
    "\n",
    "def create_transformer_model(maxlen, vocab_size):\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    #a_mask = Input(shape=(maxlen, maxlen))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x, weights = transformer_block(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    outputs = Dense(vocab_size, activation=\"sigmoid\")(x)\n",
    "    return Model(inputs=inputs, outputs=[x, outputs, weights])\n",
    "\n",
    "\n",
    "def sample_balanced(x_seqs, y_labels, ulabels_tr_dict):\n",
    "    batch_tools = list(ulabels_tr_dict.keys())\n",
    "    random.shuffle(batch_tools)\n",
    "    last_tools = batch_tools[:batch_size]\n",
    "    rand_batch_indices = list()\n",
    "    for l_tool in last_tools:\n",
    "        seq_indices = ulabels_tr_depo_tr_batch_lossict[l_tool]\n",
    "        random.shuffle(seq_indices)\n",
    "        rand_batch_indices.append(seq_indices[0])\n",
    "\n",
    "    x_batch_train = x_seqs[rand_batch_indices]\n",
    "    y_batch_train = y_labels[rand_batch_indices]\n",
    "    unrolled_x = tf.convert_to_tensor(x_batch_train, dtype=tf.int64)\n",
    "    unrolled_y = tf.convert_to_tensor(y_batch_train, dtype=tf.int64)\n",
    "    return unrolled_x, unrolled_y\n",
    "\n",
    "\n",
    "def get_u_tr_labels(y_tr):\n",
    "    labels = list()\n",
    "    labels_pos_dict = dict()\n",
    "    for i, item in enumerate(y_tr):\n",
    "        label_pos = np.where(item > 0)[0]\n",
    "        labels.extend(label_pos)\n",
    "        for label in label_pos:\n",
    "            if label not in labels_pos_dict:\n",
    "                labels_pos_dict[label] = list()\n",
    "            labels_pos_dict[label].append(i)\n",
    "\n",
    "    u_labels = list(set(labels))\n",
    "    \n",
    "    for item in labels_pos_dict:\n",
    "        labels_pos_dict[item] = list(set(labels_pos_dict[item]))\n",
    "    return u_labels, labels_pos_dict\n",
    "\n",
    "\n",
    "def sample_balanced_tr_y(x_seqs, y_labels, ulabels_tr_y_dict):\n",
    "    batch_y_tools = list(ulabels_tr_y_dict.keys())\n",
    "    random.shuffle(batch_y_tools)\n",
    "    label_tools = list()\n",
    "    rand_batch_indices = list()\n",
    "\n",
    "    for l_tool in batch_y_tools:\n",
    "        seq_indices = ulabels_tr_y_dict[l_tool]\n",
    "        random.shuffle(seq_indices)\n",
    "        \n",
    "        if seq_indices[0] not in rand_batch_indices:\n",
    "            rand_batch_indices.append(seq_indices[0])\n",
    "            label_tools.append(l_tool)\n",
    "        if len(rand_batch_indices) == batch_size:\n",
    "            break\n",
    "    \n",
    "    x_batch_train = x_seqs[rand_batch_indices]\n",
    "    y_batch_train = y_labels[rand_batch_indices]\n",
    "\n",
    "    unrolled_x = tf.convert_to_tensor(x_batch_train, dtype=tf.int64)\n",
    "    unrolled_y = tf.convert_to_tensor(y_batch_train, dtype=tf.int64)\n",
    "    return unrolled_x, unrolled_y, label_tools, rand_batch_indices\n",
    "\n",
    "\n",
    "def verify_tool_in_tr(r_dict):\n",
    "    all_sel_tool_ids = read_file(base_path + \"data/all_sel_tool_ids.txt\").split(\",\")\n",
    "\n",
    "    freq_dict = dict()\n",
    "    freq_dict_names = dict()\n",
    "\n",
    "    for tool_id in all_sel_tool_ids:\n",
    "        if tool_id not in freq_dict:\n",
    "            freq_dict[tool_id] = 0\n",
    "\n",
    "        if tool_id not in freq_dict_names:\n",
    "            freq_dict_names[r_dict[str(int(tool_id))]] = 0\n",
    "\n",
    "        freq_dict[tool_id] += 1\n",
    "        freq_dict_names[r_dict[str(int(tool_id))]] += 1\n",
    "\n",
    "    s_freq = dict(sorted(freq_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    s_freq_names = dict(sorted(freq_dict_names.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "    write_file(base_path + \"data/s_freq_names.txt\", s_freq_names)\n",
    "    write_file(base_path + \"data/s_freq.txt\", s_freq)\n",
    "\n",
    "    return s_freq\n",
    "\n",
    "\n",
    "def read_h5_model():\n",
    "    print(model_path_h5)\n",
    "    h5_path = model_path_h5 + \"model.h5\"\n",
    "    model_h5 = h5py.File(h5_path, 'r')\n",
    "\n",
    "    r_dict = json.loads(model_h5[\"reverse_dict\"][()].decode(\"utf-8\"))\n",
    "    #print(r_dict)\n",
    "    m_load_s_time = time.time()\n",
    "    #tf_loaded_model = create_transformer_model(seq_len, len(r_dict) + 1)\n",
    "    tf_loaded_model = create_transformer_model(seq_len, len(r_dict) + 1)\n",
    "    tf_loaded_model.load_weights(h5_path)\n",
    "    m_load_e_time = time.time()\n",
    "    model_loading_time = m_load_e_time - m_load_s_time\n",
    "\n",
    "    f_dict = dict((v, k) for k, v in r_dict.items())\n",
    "    c_weights = json.loads(model_h5[\"class_weights\"][()].decode(\"utf-8\"))\n",
    "    c_tools = json.loads(model_h5[\"compatible_tools\"][()].decode(\"utf-8\"))\n",
    "    s_conn = json.loads(model_h5[\"standard_connections\"][()].decode(\"utf-8\"))\n",
    "\n",
    "    model_h5.close()\n",
    "\n",
    "    return tf_loaded_model, f_dict, r_dict, c_weights, c_tools, s_conn, model_loading_time\n",
    "\n",
    "\n",
    "def read_model():\n",
    "    print(model_path)\n",
    "    m_load_s_time = time.time()\n",
    "    tf_loaded_model = tf.saved_model.load(model_path)\n",
    "    m_load_e_time = time.time()\n",
    "    m_l_time = m_load_e_time - m_load_s_time\n",
    "    r_dict = read_file(base_path + \"data/rev_dict.txt\")\n",
    "    f_dict = read_file(base_path + \"data/f_dict.txt\")\n",
    "    c_weights = read_file(base_path + \"data/class_weights.txt\")\n",
    "    c_tools = read_file(base_path + \"data/compatible_tools.txt\")\n",
    "    s_conn = read_file(base_path + \"data/published_connections.txt\")\n",
    "\n",
    "    return tf_loaded_model, f_dict, r_dict, c_weights, c_tools, s_conn, m_l_time\n",
    "    \n",
    "    \n",
    "def plot_TSNE(embed, labels):\n",
    "    print(\"Plotting embedding...\")\n",
    "    print(labels)\n",
    "\n",
    "    #perplexity = 50\n",
    "    n_colors = 10\n",
    "    figsize = (8, 8)\n",
    "\n",
    "    figure(figsize=figsize, dpi=150)\n",
    "\n",
    "    z = TSNE(n_components=2).fit_transform(embed)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"comp-1\"] = z[:,0]\n",
    "    df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=labels, data=df).set(title=\"T-SNE projection\") #palette=sns.color_palette(\"hls\", n_colors)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def predict_seq():\n",
    "\n",
    "    #visualize_loss_acc()  \n",
    "\n",
    "    #sys.exit()\n",
    "\n",
    "    #plot_model_usage_time()\n",
    "\n",
    "    #tool_tr_freq = read_file(base_path + \"data/all_sel_tool_ids.txt\")\n",
    "    #verify_training_sampling(tool_tr_freq, r_dict)  \n",
    "\n",
    "    path_test_data = base_path + \"saved_data/test.h5\"\n",
    "    \n",
    "    print(path_test_data)\n",
    "\n",
    "    file_obj = h5py.File(path_test_data, 'r')\n",
    "\n",
    "    #test_target = tf.convert_to_tensor(np.array(file_obj[\"target\"]), dtype=tf.int64)\n",
    "    test_input = np.array(file_obj[\"input\"])\n",
    "    test_target = np.array(file_obj[\"target\"])\n",
    "\n",
    "    print(test_input.shape, test_target.shape)\n",
    "\n",
    "    if predict_rnn is True:\n",
    "        print(model_path)\n",
    "        m_load_s_time = time.time()\n",
    "        tf_loaded_model = tf.saved_model.load(model_path)\n",
    "        m_load_e_time = time.time()\n",
    "        model_loading_time = m_load_e_time - m_load_s_time\n",
    "        r_dict = read_file(base_path + \"data/rev_dict.txt\")\n",
    "        f_dict = read_file(base_path + \"data/f_dict.txt\")\n",
    "        class_weights = read_file(base_path + \"data/class_weights.txt\")\n",
    "        compatible_tools = read_file(base_path + \"data/compatible_tools.txt\")\n",
    "        published_connections = read_file(base_path + \"data/published_connections.txt\")\n",
    "    else:\n",
    "        #tf_loaded_model, f_dict, r_dict, class_weights, compatible_tools, published_connections, model_loading_time = read_model()\n",
    "        tf_loaded_model, f_dict, r_dict, class_weights, compatible_tools, published_connections, model_loading_time = read_h5_model()\n",
    "\n",
    "    c_weights = list(class_weights.values())\n",
    "\n",
    "    c_weights = tf.convert_to_tensor(c_weights, dtype=tf.float32)\n",
    "\n",
    "    u_te_y_labels, u_te_y_labels_dict = get_u_tr_labels(test_target)\n",
    "\n",
    "    precision = list()\n",
    "    pub_prec_list = list()\n",
    "    error_label_tools = list()\n",
    "    batch_pred_time = list()\n",
    "    for j in range(test_batches):\n",
    "\n",
    "        te_x_batch, y_train_batch, selected_label_tools, bat_ind = sample_balanced_tr_y(test_input, test_target, u_te_y_labels_dict)\n",
    "\n",
    "        #print(j * batch_size, j * batch_size + batch_size)\n",
    "        #te_x_batch = test_input[j * batch_size : j * batch_size + batch_size, :]\n",
    "        #y_train_batch = test_target[j * batch_size : j * batch_size + batch_size, :]\n",
    "\n",
    "        te_x_batch = tf.cast(te_x_batch, dtype=tf.float32, name=\"input_2\")\n",
    "        \n",
    "        pred_s_time = time.time()\n",
    "        \n",
    "        if predict_rnn is True:\n",
    "            te_prediction = tf_loaded_model(te_x_batch, training=False)\n",
    "        else:\n",
    "            embed, te_prediction, att_weights = tf_loaded_model(te_x_batch, training=False)\n",
    "            print(embed.shape, te_prediction.shape, att_weights.shape)\n",
    "           \n",
    "        pred_e_time = time.time()\n",
    "        diff_time = (pred_e_time - pred_s_time) / float(batch_size)\n",
    "        batch_pred_time.append(diff_time)\n",
    "        filter_embed = list()\n",
    "        filter_embed_label = list()\n",
    "        filter_embed_label_names = list()\n",
    "        for i, (inp, tar) in enumerate(zip(te_x_batch, y_train_batch)):\n",
    "\n",
    "            t_ip = te_x_batch[i]\n",
    "            tar = y_train_batch[i]\n",
    "            prediction = te_prediction[i]\n",
    "            if len(np.where(inp > 0)[0]) <= max_seq_len:\n",
    "            #if len(np.where(tar > 0)[0]) == 1:\n",
    "                print(tar, len(np.where(tar > 0)[0]))\n",
    "                real_prediction = np.where(tar > 0)[0]\n",
    "                target_pos = real_prediction #list(set(all_tr_label_tools_ids).intersection(set(real_prediction)))\n",
    "\n",
    "                prediction_wts = tf.math.multiply(c_weights, prediction)\n",
    "                filter_embed.append(embed[i])\n",
    "                n_topk = len(target_pos)\n",
    "                top_k = tf.math.top_k(prediction, k=n_topk, sorted=True)\n",
    "                print(i, top_k.indices.numpy())\n",
    "                top_k_wts = tf.math.top_k(prediction_wts, k=n_topk, sorted=True)\n",
    "\n",
    "                t_ip = t_ip.numpy()\n",
    "                label_pos = np.where(t_ip > 0)[0]\n",
    "                \n",
    "                one_target_pos = target_pos[np.random.randint(len(target_pos))]\n",
    "                filter_embed_label_names.append(r_dict[str(one_target_pos)])\n",
    "                filter_embed_label.append(str(one_target_pos))\n",
    "                \n",
    "                i_names = \",\".join([r_dict[str(int(item))] for item in t_ip[label_pos]  if item not in [0, \"0\"]])\n",
    "                t_names = \",\".join([r_dict[str(int(item))] for item in target_pos  if item not in [0, \"0\"]])\n",
    "\n",
    "                last_i_tool = [r_dict[str(int(item))] for item in t_ip[label_pos]][-1]\n",
    "\n",
    "                true_tools = [r_dict[str(int(item))] for item in target_pos]\n",
    "\n",
    "                pred_tools = [r_dict[str(int(item))] for item in top_k.indices.numpy()  if item not in [0, \"0\"]]\n",
    "                pred_tools_wts = [r_dict[str(int(item))] for item in top_k_wts.indices.numpy()  if item not in [0, \"0\"]]\n",
    "\n",
    "                intersection = list(set(true_tools).intersection(set(pred_tools)))\n",
    "\n",
    "                pub_prec = 0.0\n",
    "                pub_prec_wt = 0.0\n",
    "\n",
    "                if last_i_tool in published_connections:\n",
    "                    true_pub_conn = published_connections[last_i_tool]\n",
    "\n",
    "                    if len(pred_tools) > 0:\n",
    "                        intersection_pub = list(set(true_pub_conn).intersection(set(pred_tools)))\n",
    "                        intersection_pub_wt = list(set(true_pub_conn).intersection(set(pred_tools_wts)))\n",
    "                        pub_prec = float(len(intersection_pub)) / len(pred_tools)\n",
    "                        pub_prec_list.append(pub_prec)\n",
    "                        pub_prec_wt = float(len(intersection_pub_wt)) / len(pred_tools)\n",
    "                    else:\n",
    "                        pub_prec = False\n",
    "                        pub_prec_wt = False\n",
    "\n",
    "                if len(pred_tools) > 0:\n",
    "                    pred_precision = float(len(intersection)) / len(pred_tools)\n",
    "                    precision.append(pred_precision)\n",
    "\n",
    "                if pred_precision < 2.0:\n",
    "            \n",
    "                    print(\"Test batch {}, Tool sequence: {}\".format(j+1, [r_dict[str(int(item))] for item in t_ip[label_pos]]))\n",
    "                    print()\n",
    "                    print(\"Test batch {}, True tools: {}\".format(j+1, true_tools))\n",
    "                    print()\n",
    "                    print(\"Test batch {}, Predicted top {} tools: {}\".format(j+1, n_topk, pred_tools))\n",
    "                    print()\n",
    "                    print(\"Test batch {}, Predicted top {} tools with weights: {}\".format(j+1, n_topk, pred_tools_wts))\n",
    "                    print()\n",
    "                    print(\"Test batch {}, Precision: {}\".format(j+1, pred_precision)) \n",
    "                    print()\n",
    "                    print(\"Test batch {}, Published precision: {}\".format(j+1, pub_prec))\n",
    "                    print()\n",
    "                    print(\"Test batch {}, Published precision with weights: {}\".format(j+1, pub_prec_wt))\n",
    "                    print()\n",
    "                    print(\"Time taken to predict tools: {} seconds\".format(diff_time))\n",
    "                    print(\"=========================\")\n",
    "                print(\"--------------------------\")\n",
    "                generated_attention(att_weights[i], i_names, f_dict, r_dict)\n",
    "                print(\"Batch {} prediction finished ...\".format(j+1))\n",
    "\n",
    "    plot_TSNE(np.array(filter_embed), filter_embed_label_names)\n",
    "    sys.exit()\n",
    "    \n",
    "    te_lowest_t_ids = read_file(base_path + \"data/te_lowest_t_ids.txt\")\n",
    "    lowest_t_ids = [int(item) for item in te_lowest_t_ids.split(\",\")]\n",
    "    print(lowest_t_ids)\n",
    "    lowest_t_ids = lowest_t_ids[:1]\n",
    "    \n",
    "    low_te_data = test_input[lowest_t_ids]\n",
    "    low_te_labels = test_target[lowest_t_ids]\n",
    "    low_te_data = tf.cast(low_te_data, dtype=tf.float32)\n",
    "    low_topk = 20\n",
    "    low_te_precision = list()\n",
    "    low_te_pred_time = list()\n",
    "\n",
    "    pred_s_time = time.time()\n",
    "    if predict_rnn is True:\n",
    "        bat_low_prediction = tf_loaded_model(low_te_data, training=False)\n",
    "    else:\n",
    "        bat_embed_low, bat_low_prediction, att_weights = tf_loaded_model(low_te_data, training=False)\n",
    "        print(bat_embed_low.shape, bat_low_prediction.shape, att_weights.shape)\n",
    "    pred_e_time = time.time()\n",
    "    low_diff_pred_t = (pred_e_time - pred_s_time) / float(len(lowest_t_ids))\n",
    "    low_te_pred_time.append(low_diff_pred_t)\n",
    "    print(\"Time taken to predict tools: {} seconds\".format(low_diff_pred_t))\n",
    "\n",
    "    for i, (low_inp, low_tar) in enumerate(zip(low_te_data, low_te_labels)):\n",
    "\n",
    "        low_prediction = bat_low_prediction[i]\n",
    "        low_tar = low_te_labels[i]\n",
    "        low_label_pos = np.where(low_tar > 0)[0]\n",
    "\n",
    "        low_topk = len(low_label_pos)\n",
    "        low_topk_pred = tf.math.top_k(low_prediction, k=low_topk, sorted=True)\n",
    "        low_topk_pred = low_topk_pred.indices.numpy()\n",
    "        \n",
    "        low_label_pos_tools = [r_dict[str(int(item))] for item in low_label_pos if item not in [0, \"0\"]]\n",
    "        low_pred_label_pos_tools = [r_dict[str(int(item))] for item in low_topk_pred if item not in [0, \"0\"]]\n",
    "\n",
    "        low_intersection = list(set(low_label_pos_tools).intersection(set(low_pred_label_pos_tools)))\n",
    "        low_pred_precision = float(len(low_intersection)) / len(low_label_pos)\n",
    "        low_te_precision.append(low_pred_precision)\n",
    "\n",
    "        low_inp_pos = np.where(low_inp > 0)[0]\n",
    "        low_inp = low_inp.numpy()\n",
    "        print(low_inp, low_inp_pos)\n",
    "        print(\"{}, Low: test tool sequence: {}\".format(i, [r_dict[str(int(item))] for item in low_inp[low_inp_pos]]))\n",
    "        print()\n",
    "        print(\"{},Low: True labels: {}\".format(i, low_label_pos_tools))\n",
    "        print()\n",
    "        print(\"{},Low: Predicted labels: {}, Precision: {}\".format(i, low_pred_label_pos_tools, low_pred_precision))\n",
    "       \n",
    "        print(\"-----------------\")\n",
    "        print()\n",
    "\n",
    "    if test_batches > 0:\n",
    "        print(\"Batch Precision@{}: {}\".format(n_topk, np.mean(precision)))\n",
    "        print(\"Batch Published Precision@{}: {}\".format(n_topk, np.mean(pub_prec_list)))\n",
    "        print(\"Batch Trained model loading time: {} seconds\".format(model_loading_time))\n",
    "        print(\"Batch average seq pred time: {} seconds\".format(np.mean(batch_pred_time)))\n",
    "        print(\"Batch total model loading and pred time: {} seconds\".format(model_loading_time + np.mean(batch_pred_time)))\n",
    "        print()\n",
    "        \n",
    "    print(\"----------------------------\")\n",
    "    print()\n",
    "    print(\"Predicting for individual sequences...\")\n",
    "    print()\n",
    "    print(\"Predicting for individual tools or sequences\")\n",
    "    n_topk_ind = 20\n",
    "    t_ip = np.zeros((1, 25))\n",
    "\n",
    "    t_ip[0, 0] = int(f_dict[\"bowtie2\"])\n",
    "    t_ip[0, 1] = int(f_dict[\"hicexplorer_hicbuildmatrix\"])\n",
    "    t_ip[0, 2] = int(f_dict[\"hicexplorer_chicqualitycontrol\"])\n",
    "    t_ip[0, 3] = int(f_dict[\"hicexplorer_chicviewpointbackgroundmodel\"])\n",
    "    t_ip[0, 4] = int(f_dict[\"hicexplorer_chicviewpoint\"])\n",
    "    \n",
    "    last_tool_name = \"hicexplorer_chicviewpoint\"\n",
    "    \n",
    "    t_ip = tf.convert_to_tensor(t_ip, dtype=tf.int64)\n",
    "    t_ip = tf.cast(t_ip, dtype=tf.float32)\n",
    "    \n",
    "    pred_s_time = time.time()\n",
    "    if predict_rnn is True:\n",
    "        prediction = tf_loaded_model(t_ip, training=False)\n",
    "    else:\n",
    "        indi_embed, prediction, att_weights = tf_loaded_model(t_ip, training=False)\n",
    "        print(indi_embed.shape, prediction.shape, att_weights.shape)\n",
    "    pred_e_time = time.time()\n",
    "    print(\"Time taken to predict tools: {} seconds\".format(pred_e_time - pred_s_time))\n",
    "    prediction_cwts = tf.math.multiply(c_weights, prediction)\n",
    "\n",
    "    top_k = tf.math.top_k(prediction, k=n_topk_ind, sorted=True)\n",
    "    top_k_wts = tf.math.top_k(prediction_cwts, k=n_topk_ind, sorted=True)\n",
    "\n",
    "    t_ip = t_ip.numpy()[0]\n",
    "    label_pos = np.where(t_ip > 0)[0]\n",
    "    print(t_ip)\n",
    "    print(t_ip.shape, t_ip[label_pos])\n",
    "    i_names = \",\".join([r_dict[str(int(item))] for item in t_ip[label_pos] if item not in [0, \"0\"]])\n",
    "\n",
    "    pred_tools = [r_dict[str(int(item))] for item in top_k.indices.numpy()[0] if item not in [0, \"0\"]]\n",
    "    pred_tools_wts = [r_dict[str(int(item))] for item in top_k_wts.indices.numpy()[0] if item not in [0, \"0\"]]\n",
    "\n",
    "    c_tools = []\n",
    "    if str(f_dict[last_tool_name]) in compatible_tools:\n",
    "        c_tools = [r_dict[str(item)] for item in compatible_tools[str(f_dict[last_tool_name])]]\n",
    "\n",
    "    pred_intersection = list(set(pred_tools).intersection(set(c_tools)))\n",
    "    prd_te_prec = len(pred_intersection) / float(n_topk_ind)\n",
    "\n",
    "    print(\"Tool sequence: {}\".format([r_dict[str(int(item))] for item in t_ip[label_pos]]))\n",
    "    print()\n",
    "    print(\"Compatible true tools: {}, size: {}\".format(c_tools, len(c_tools)))\n",
    "    print()\n",
    "    print(\"Predicted top {} tools: {}\".format(n_topk_ind, pred_tools))\n",
    "    print()\n",
    "    print(\"Predicted precision: {}\".format(prd_te_prec))\n",
    "    print()\n",
    "    print(\"Correctly predicted tools: {}\".format(pred_intersection))\n",
    "    print()\n",
    "    print(\"Predicted top {} tools with weights: {}\".format(n_topk_ind, pred_tools_wts))\n",
    "    print()\n",
    "    if predict_rnn is False:\n",
    "        generated_attention(att_weights, i_names, f_dict, r_dict)\n",
    "\n",
    "\n",
    "def generated_attention(attention_weights, i_names, f_dict, r_dict):\n",
    "    try:\n",
    "        attention_heads = tf.squeeze(attention_weights, 0)\n",
    "    except:\n",
    "        attention_heads = attention_weights\n",
    "    n_heads = attention_heads.shape[1]\n",
    "    i_names = i_names.split(\",\")\n",
    "    in_tokens = i_names\n",
    "    out_tokens = i_names\n",
    "    \n",
    "    mean_att = np.mean(attention_heads, axis=0)\n",
    "    for h, head in enumerate(attention_heads):\n",
    "      plot_attention_head(in_tokens, out_tokens, head)\n",
    "      break\n",
    "\n",
    "\n",
    "def plot_attention_head(in_tokens, out_tokens, attention):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  ax = plt.gca()\n",
    "  cax = ax.matshow(attention[:len(in_tokens), :len(out_tokens)], interpolation='nearest')\n",
    "  ax.set_xlabel(f'Head')\n",
    "\n",
    "  ax.set_xticks(range(len(in_tokens)))\n",
    "  ax.set_xticklabels(in_tokens, rotation=90)\n",
    "\n",
    "  ax.set_yticks(range(len(out_tokens)))\n",
    "  ax.set_yticklabels(out_tokens)\n",
    "  fig.colorbar(cax)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_attention_head_axes(att_weights):\n",
    "    seq_len = 25\n",
    "    n_heads = 4\n",
    "    attention_heads = tf.squeeze(att_weights, 0)\n",
    "    attention_heads = attention_heads.numpy()\n",
    "    print(attention_heads.shape)\n",
    "    #print(attention_heads[0:, 0:])\n",
    "    att_flatten = attention_heads.flatten()\n",
    "    print(att_flatten.shape)\n",
    "    block1 = att_flatten[0: seq_len * n_heads]\n",
    "    block1 = block1.reshape((seq_len, n_heads))\n",
    "    print(block1.shape)\n",
    "    \n",
    "    block2 = att_flatten[attention_heads.shape[0] * attention_heads.shape[1]: ]\n",
    "    block2 = block2.reshape((attention_heads.shape[0], attention_heads.shape[1]))\n",
    "    print(block2.shape)\n",
    "    plt.matshow(block1)\n",
    "    plt.matshow(block2)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf440a6b-37c4-4e14-ac55-10c18aff8107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/transformer/saved_data/test.h5\n"
     ]
    }
   ],
   "source": [
    "predict_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8684f8c-d72d-4fc7-8624-d42666a41d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mf_dict\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f70d1a-4f98-421d-abb4-b53a01bd4d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7156a5a-e353-4c21-9141-26a5e5995ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f474443-0a97-4f2a-98d0-6c943eb77821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f478ba-b227-4088-8d0d-f205c729c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Tool seqs for good attention plots:\n",
    "'schicexplorer_schicqualitycontrol', 'schicexplorer_schicnormalize', 'schicexplorer_schicclustersvl'\n",
    "\n",
    "\n",
    "# Tested tools: porechop, schicexplorer_schicqualitycontrol, schicexplorer_schicclustersvl, snpeff_sars_cov_2\n",
    "    # sarscov2genomes, ivar_covid_aries_consensus, remove_nucleotide_deletions, pangolin\n",
    "    # bowtie2,lofreq_call\n",
    "    # dropletutils_read_10x\n",
    "    # 'bowtie2', 'hicexplorer_hicbuildmatrix'\n",
    "    # 'mtbls520_04_preparations', 'mtbls520_05a_import_maf', 'mtbls520_06_import_traits', 'mtbls520_07_species_diversity'\n",
    "    # ctsm_fates: 'xarray_metadata_info', 'interactive_tool_panoply', 'xarray_select', '__EXTRACT_DATASET__'\n",
    "    # msnbase_readmsdata: 'abims_xcms_xcmsSet', 'xcms_export_samplemetadata', 'xcms_plot_chromatogram'\n",
    "    # ncbi_eutils_esearch: ncbi_eutils_elink\n",
    "    # 1_create_conf: '5_calc_stat', '4_filter_sam', '2_map', 'conf4circos', '3_filter_single_pair'\n",
    "    # pdaug_peptide_data_access: pdaug_tsvtofasta\n",
    "    # 'pdaug_peptide_data_access', 'pdaug_tsvtofasta': 'pdaug_peptide_sequence_analysis', 'pdaug_fishers_plot', 'pdaug_sequence_property_based_descriptors'\n",
    "    # 'rankprodthree', 'Remove beginning1', 'cat1', 'Cut1', 'interactions': 'biotranslator', 'awkscript'\n",
    "    # rpExtractSink: rpCompletion', 'retropath2'\n",
    "    # 'EMBOSS: transeq101', 'ncbi_makeblastdb', 'ncbi_blastp_wrapper', 'blast_parser', 'hcluster_sg'\n",
    "    # 'Remove beginning1', 'Cut1', 'param_value_from_file', 'kc-align', 'sarscov2formatter', 'hyphy_fel'\n",
    "    # abims_CAMERA_annotateDiffreport\n",
    "    # cooler_csort_pairix\n",
    "    # mycrobiota-split-multi-otutable_ensembl_gtf2gene_list\n",
    "    # XY_Plot_1\n",
    "    # mycrobiota-qc-report\n",
    "    # 1_create_conf\n",
    "    # RNAlien\n",
    "    # ont_fast5_api_multi_to_single_fast5 \n",
    "    # ctb_remIons\n",
    "\n",
    "    Incorrect predictions\n",
    "    # scpipe, \n",
    "    # 'delly_call', 'delly_merge'ivar_covid_aries_consensus\n",
    "    # 'gmap_build', 'gsnap', 'sam_to_bam', 'filter', 'assign', 'polyA'\n",
    "    # 'bioext_bealign', 'tn93_filter', 'hyphy_cfel'\n",
    "    # sklearn_build_pipeline\n",
    "    # split_file_to_collection', 'rdock_rbdock', 'xchem_pose_scoring', 'sucos_max_score'\n",
    "    # 'rmcontamination', 'scaffold2fasta'  \n",
    "    # 'rmcontamination', 'scaffold2fasta'\n",
    "    # cat1', 'fastq_filter', 'cshl_fastq_to_fasta', 'filter_16s_wrapper_script 1'\n",
    "    # 'TrimPrimer', 'Flash', 'Btrim64', 'uparse'\n",
    "    # 'cshl_fastq_to_fasta', 'cshl_fastx_trimmer', 'fasta_tabular_converter\n",
    "    # CryptoGenotyper\n",
    "    # cooler_makebins\n",
    "    # 'PeakPickerHiRes', 'FileFilter', 'xcms-find-peaks', 'xcms-collect-peaks'\n",
    "    # 'TrimPrimer', 'Flash', 'Btrim64'\n",
    "    # cryptotyperanndata_import\n",
    "    # ip_spot_detection_2d\n",
    "    # 'picard_FastqToSam', 'TagBamWithReadSequenceExtended', 'FilterBAM', 'BAMTagHistogram'\n",
    "    # 'basic_illumination', 'ashlar'\n",
    "    # 'cghub_genetorrent', 'gatk_indel'\n",
    "    # 'FeatureFinderMultiplex', 'HighResPrecursorMassCorrector', 'MSGFPlusAdapter', 'PeptideIndexer', 'IDMerger', 'ConsensusID'\n",
    "    # 'PeakPickerHiRes', 'FileFilter', 'xcms-find-peaks', 'xcms-collect-peaks'\n",
    "    # 'PeakPickerHiRes', 'FileFilter', 'xcms-find-peaks', 'xcms-collect-peaks', 'xcms-group-peaks', 'xcms-blankfilter', 'xcms-dilutionfilter', 'camera-annotate-peaks', 'camera-group-fwhm', 'camera-find-adducts', 'camera-find-isotopes'\n",
    "    # 'minfi_read450k', 'minfi_mset'\n",
    "    # 'msnbase_readmsdata', 'abims_xcms_xcmsSet', 'abims_xcms_refine'\n",
    "    # # 'snpEff_build_gb', 'bwa_mem', 'samtools_view',\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "def predict_seq():\n",
    "\n",
    "\n",
    "    #sys.exit()\n",
    "    # read test sequences\n",
    "    r_dict = read_file(base_path + \"data/rev_dict.txt\")\n",
    "    f_dict = read_file(base_path + \"data/f_dict.txt\")\n",
    "    \n",
    "    tf_loaded_model = tf.saved_model.load(model_path)\n",
    "    #predictor = predict_sequences.PredictSequence(tf_loaded_model)\n",
    "\n",
    "    #predictor(test_input, test_target, f_dict, r_dict)\n",
    "\n",
    "    #tool_name = \"cutadapt\"\n",
    "    #print(\"Prediction for {}...\".format(tool_name))\n",
    "    bowtie_output = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    bowtie_output = bowtie_output.write(0, [tf.constant(index_start_token, dtype=tf.int64)])\n",
    "    #bowtie_output = bowtie_output.write(1, [tf.constant(295, dtype=tf.int64)])\n",
    "    bowtie_o = tf.transpose(bowtie_output.stack())\n",
    "    #tool_id = f_dict[tool_name]\n",
    "    #print(tool_name, tool_id)\n",
    "    tool_list = [\"ctb_filter\"]\n",
    "    bowtie_input = np.zeros([1, 25])\n",
    "    bowtie_input[:, 0] = index_start_token\n",
    "    bowtie_input[:, 1] = f_dict[tool_list[0]]\n",
    "    #bowtie_input[:, 2] = f_dict[tool_list[1]]\n",
    "    #bowtie_input[:, 3] = f_dict[\"featurecounts\"]\n",
    "    #bowtie_input[:, 4] = f_dict[\"deseq2\"]\n",
    "    bowtie_input = tf.constant(bowtie_input, dtype=tf.int64)\n",
    "    print(bowtie_input, bowtie_output, bowtie_o)\n",
    "    bowtie_pred, _ = tf_loaded_model([bowtie_input, bowtie_o], training=False)\n",
    "    print(bowtie_pred.shape)\n",
    "    top_k = tf.math.top_k(bowtie_pred, k=10)\n",
    "    print(\"Top k: \", bowtie_pred.shape, top_k, top_k.indices)\n",
    "    print(np.all(top_k.indices.numpy(), axis=-1))\n",
    "    print(\"Predicted tools for {}: {}\".format( \",\".join(tool_list), [r_dict[str(item)] for item in top_k.indices.numpy()[0][0]]))\n",
    "    print()\n",
    "    #print(\"Generating predictions...\")\n",
    "    #generated_attention(tf_loaded_model, f_dict, r_dict)\n",
    "\n",
    "\n",
    "def generated_attention(trained_model, f_dict, r_dict):\n",
    "\n",
    "    np_output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    np_output_array = np_output_array.write(0, [tf.constant(index_start_token, dtype=tf.int64)])\n",
    "\n",
    "    n_target_items = 5\n",
    "    n_input = np.zeros([1, 25])\n",
    "    n_input[:, 0] = index_start_token\n",
    "    n_input[:, 1] = f_dict[\"hicexplorer_hicadjustmatrix\"]\n",
    "    #n_input[:, 2] = f_dict[\"hicexplorer_hicbuildmatrix\"]\n",
    "    #n_input[:, 3] = f_dict[\"hicexplorer_hicfindtads\"]\n",
    "    #n_input[:, 4] = f_dict[\"deseq2\"]\n",
    "    #n_input[:, 5] = f_dict[\"Add_a_column1\"]\n",
    "    #n_input[:, 6] = f_dict[\"table_compute\"]\n",
    "    a_input = n_input\n",
    "    n_input = tf.constant(n_input, dtype=tf.int64)\n",
    "   \n",
    "    for i in range(n_target_items):\n",
    "        #print(i, index)\n",
    "        output = tf.transpose(np_output_array.stack())\n",
    "        print(\"decoder input: \", n_input, output, output.shape)\n",
    "        orig_predictions, _ = trained_model([n_input, output], training=False)\n",
    "        #print(orig_predictions.shape)trimmomatic\n",
    "        #print(\"true target seq real: \", te_tar_real)\n",
    "        #print(\"Pred seq argmax: \", tf.argmax(orig_predictions, axis=-1))\n",
    "        predictions = orig_predictions[:, -1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "        np_output_array = np_output_array.write(i+1, predicted_id[0])\n",
    "    print(output, np_output_array.stack(), output.numpy())\n",
    "    print(\"----------\")\n",
    "    last_decoder_layer = \"decoder_layer4_block2\"\n",
    "    _, attention_weights = trained_model([n_input, output[:,:-1]], training=False)\n",
    "    pred_attention = attention_weights[last_decoder_layer]\n",
    "\n",
    "    print(attention_weights[last_decoder_layer].shape)\n",
    "    head = 0\n",
    "    attention_heads = tf.squeeze(attention_weights[last_decoder_layer], 0)\n",
    "    pred_attention = attention_heads[head]\n",
    "    print(pred_attention)\n",
    "\n",
    "    #print(attention_weights)\n",
    "    in_tokens = [r_dict[str(int(item))] for itscanpy_read_10xem in a_input[0] if item > 0]\n",
    "    out_tokens = [r_dict[str(int(item))] for item in output.numpy()[0]]\n",
    "    out_tokens = out_tokens[1:]\n",
    "    print(in_tokens)\n",
    "    print(out_tokens)\n",
    "    pred_attention = pred_attention[:,:len(in_tokens)]\n",
    "    print(pred_attention)\n",
    "    plot_attention_head(in_tokens, out_tokens, pred_attention)\n",
    "\n",
    "scanpy_read_10x\n",
    "def plot_attention_head(in_tokens, out_tokens, attention):\n",
    "  # The plot is of the attention when a token walog_19_09_22_GPU_transformer_full_datas generated.\n",
    "  # The model didn't generate `<START>` in the output. Skip it.\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111)\n",
    "  cax = ax.matshow(attention, interpolation='nearest')ctb_chemfp_nxn_clustering\n",
    "  fig.colorbar(cax)\n",
    "\n",
    "  #ax = plt.gca()\n",
    "  #ax.matshow(attention)\n",
    "\n",
    "  ax.set_xticks(range(len(in_tokens)))\n",
    "  ax.set_yticks(range(len(out_tokens)))\n",
    "\n",
    "  ax.set_xticklabels(in_tokens, rotation=90)\n",
    "  ax.set_yticklabels(out_tokens)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
